{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    database=\"amazon_data\",\n",
    "    connect_timeout=1000\n",
    ")\n",
    "\n",
    "cursor = mydb.cursor()\n",
    "print(\"connection created\")\n",
    "only_changing_issue_ids = []\n",
    "weekly_counts = {}\n",
    "monthly_counts = {}\n",
    "changed_issue_ids_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from the database and create dataframes for each issue ID\n",
    "def fetch_data_grouped_by_issue_ids():\n",
    "    query = (\n",
    "        \"SELECT \"\n",
    "        \"   `Status`, `Notes`, `Issue type`, `Issue ID`, `Infraction Subtype Code`, \"\n",
    "        \"   `Reversal Invoice #`, `Charge Invoice #`, `Financial charge`, `scrape_date` \"\n",
    "        \"FROM csv_data_scrape_date \"\n",
    "        \"WHERE `Issue ID` IN (\"\n",
    "        \"   SELECT `Issue ID` \"\n",
    "        \"   FROM csv_data_scrape_date \"\n",
    "        \"   GROUP BY `Issue ID` \"\n",
    "        \"   HAVING COUNT(*) > 2\"\n",
    "        \") \"\n",
    "    )\n",
    "    print(\"Fetching data from DB\")\n",
    "\n",
    "    data_frames_dict = {}  # Dictionary to store dataframes for each issue ID\n",
    "    data_frames_dict_without_scrape = {}  # Dictionary without scrape_date column\n",
    "    cursor.execute(query)\n",
    "    all_data = cursor.fetchall()\n",
    "    column_names = [col[0] for col in cursor.description]\n",
    "\n",
    "    print(\"query executed\")\n",
    "    for row in all_data:\n",
    "        issue_id_index = column_names.index('Issue ID')\n",
    "        issue_id = row[issue_id_index]\n",
    "        if issue_id not in data_frames_dict:\n",
    "            data_frames_dict[issue_id] = []\n",
    "\n",
    "        data_frames_dict[issue_id].append(row)\n",
    "\n",
    "        # Create a row without the 'scrape_date' column\n",
    "        row_without_scrape = [row[i] for i, col_name in enumerate(column_names) if col_name != 'scrape_date']\n",
    "        if issue_id not in data_frames_dict_without_scrape:\n",
    "            data_frames_dict_without_scrape[issue_id] = []\n",
    "\n",
    "        data_frames_dict_without_scrape[issue_id].append(row_without_scrape)\n",
    "\n",
    "    mydb.close()\n",
    "\n",
    "    return data_frames_dict, data_frames_dict_without_scrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_chunks, all_data_chunks_without_date = fetch_data_grouped_by_issue_ids()\n",
    "print(\"Total issue ids : \", len(all_data_chunks), len(all_data_chunks_without_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = [\n",
    "    \"`Status`\", \"`Notes`\", \"`Issue type`\", \"`Issue ID`\", \"`Infraction Subtype Code`\",\n",
    "    \"`Reversal Invoice #`\", \"`Charge Invoice #`\", \"`Financial charge`\", \"`scrape_date`\"\n",
    "]\n",
    "\n",
    "cols_to_check_without_date = [\n",
    "    \"`Status`\", \"`Notes`\", \"`Issue type`\", \"`Issue ID`\", \"`Infraction Subtype Code`\",\n",
    "    \"`Reversal Invoice #`\", \"`Charge Invoice #`\", \"`Financial charge`\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunks(data_frames_dict):\n",
    "\n",
    "    with open('real_data_output_GT2_complete.txt', 'w') as log_file:\n",
    "    \n",
    "        count = 1\n",
    "    \n",
    "        column_total_change_sums = {col: 0 for col in cols_to_check_without_date}\n",
    "        column_total_days_to_change = {col: 0 for col in cols_to_check_without_date}\n",
    "\n",
    "        for index, rows_list in data_frames_dict.items():\n",
    "            \n",
    "            if (count % 20000 == 0):\n",
    "                print(\"Reached iteration : \", count)        \n",
    "            count = count + 1\n",
    "\n",
    "            returned_specific_issue_data = pd.DataFrame(rows_list, columns=cols_to_check)\n",
    "\n",
    "            column_avg_change_sums = {col: 0 for col in cols_to_check_without_date}\n",
    "            column_change_days = {col: [] for col in cols_to_check_without_date}\n",
    "\n",
    "            for j in range(1, len(returned_specific_issue_data) ):\n",
    "            \n",
    "                prev_row = returned_specific_issue_data.iloc[j - 1]\n",
    "                current_row = returned_specific_issue_data.iloc[j]\n",
    "\n",
    "                for col in cols_to_check_without_date:\n",
    "            \n",
    "                    if col != '`scrape_date`':  # Exclude 'scrape_date' from column change calculations\n",
    "            \n",
    "                        if prev_row[col] != current_row[col]:\n",
    "                        \n",
    "                            column_avg_change_sums[col] += 1\n",
    "                            days_difference = (pd.to_datetime(current_row['`scrape_date`']) - pd.to_datetime(prev_row['`scrape_date`'])).days\n",
    "                            column_change_days[col].append((prev_row[col], current_row[col], days_difference))\n",
    "\n",
    "            total_changes = sum(column_avg_change_sums.values())\n",
    "\n",
    "            if total_changes > 0:\n",
    "\n",
    "                only_changing_issue_ids.append(index)\n",
    "                print(\"************************************************\", file=log_file)\n",
    "                print(\"Changed columns for id : \", index, \" Rows : \", len(rows_list), \"\\n\", file=log_file)\n",
    "                log_file.flush()\n",
    "\n",
    "                # Print the column name, previous and next values, and total changes\n",
    "                for col in cols_to_check_without_date:\n",
    "\n",
    "                    if col != '`scrape_date`':  # Exclude 'scrape_date' from output\n",
    "                        print(f\"{col} ({column_avg_change_sums[col]}):\", file=log_file)\n",
    "                    \n",
    "                        if col in column_change_days and len(column_change_days[col]) > 0:\n",
    "                            for prev_val, next_val, days_diff in column_change_days[col]:\n",
    "                                print(f\"\\t{prev_val} -> {next_val} (Change took {days_diff} days)\", file=log_file)\n",
    "                        \n",
    "                        log_file.flush()\n",
    "\n",
    "                        # Accumulate the column change sums and days to change\n",
    "                        column_total_change_sums[col] += column_avg_change_sums[col]\n",
    "                        if len(column_change_days[col]) > 0:\n",
    "                            column_total_days_to_change[col] += sum(days_diff for _, _, days_diff in column_change_days[col]) / len(column_change_days[col])\n",
    "\n",
    "                print(\"\\nTotal changes:\", total_changes, file=log_file)\n",
    "                log_file.flush()\n",
    "\n",
    "                # Display average days to change for each column for the current ID\n",
    "                print(\"\\nAverage Days to Change for Each Column (ID:\", index, \"):\", file=log_file)\n",
    "                log_file.flush()\n",
    "                for col in cols_to_check_without_date:\n",
    "                    if col != '`scrape_date`':  # Exclude 'scrape_date' from output\n",
    "                        avg_days_to_change = column_total_days_to_change[col] / len(column_change_days[col]) if len(column_change_days[col]) > 0 else 0\n",
    "                        print(\"\\t\", col, \":\", avg_days_to_change, \" days to change\", file=log_file)\n",
    "\n",
    "        # Write total sum of changes for each column at the end of the file\n",
    "        print(\"************************************************\", file=log_file)\n",
    "        print(\"Total Sum of Changes for Each Column:\", file=log_file)\n",
    "        for col, total_change_sum in column_total_change_sums.items():\n",
    "            if col != '`scrape_date`':\n",
    "                print(f\"\\t{col}:\", total_change_sum, \"changes\", file=log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_chunks(all_data_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Only changing issue ids : \",len(only_changing_issue_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_changing_issue_ids(data_frames_dict, issue_ids_to_filter):\n",
    "    changed_issue_ids = {issue_id: data_frames_dict[issue_id] for issue_id in issue_ids_to_filter}\n",
    "    return changed_issue_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_issue_ids_dict = filter_changing_issue_ids(all_data_chunks, only_changing_issue_ids)\n",
    "print(len(changed_issue_ids_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_report(changed_issue_ids_dict):\n",
    "    weekly_counts = {}  # Dictionary to store weekly counts of changes\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    for index, rows_list in changed_issue_ids_dict.items():\n",
    "        if count % 20000 == 0:\n",
    "            print(\"Iteration reached:\", count)\n",
    "        count = count + 1\n",
    "\n",
    "        returned_specific_issue_data = pd.DataFrame(rows_list, columns=cols_to_check)\n",
    "\n",
    "        # Convert the 'scrape_date' column to datetime format\n",
    "        returned_specific_issue_data['`scrape_date`'] = pd.to_datetime(returned_specific_issue_data['`scrape_date`'])\n",
    "\n",
    "        # Group the data by week\n",
    "        grouped = returned_specific_issue_data.groupby(returned_specific_issue_data['`scrape_date`'].dt.to_period('W'))\n",
    "\n",
    "        # Iterate through each week and its corresponding grouped data\n",
    "        for week, group_data in grouped:\n",
    "            week_str = str(week)\n",
    "            \n",
    "            # Create a dictionary entry for the current week if it doesn't exist\n",
    "            if week_str not in weekly_counts:\n",
    "                weekly_counts[week_str] = {col: 0 for col in cols_to_check_without_date}\n",
    "            \n",
    "            # Calculate the changes in columns for the current week\n",
    "            column_changes = {col: 0 for col in cols_to_check_without_date}\n",
    "            \n",
    "            for idx in range(1, len(group_data)):  # Start from index 1 to skip the first row\n",
    "                prev_row = group_data.iloc[idx - 1]\n",
    "                row = group_data.iloc[idx]\n",
    "                for col in cols_to_check_without_date:\n",
    "                    if col != '`scrape_date`' and prev_row[col] != row[col]:\n",
    "                        column_changes[col] += 1\n",
    "            \n",
    "            # Update the weekly counts with the changes for this week\n",
    "            for col in cols_to_check_without_date:\n",
    "                weekly_counts[week_str][col] += column_changes[col]\n",
    "\n",
    "    with open('real_data_output_GT2_complete.txt', 'a') as log_file:\n",
    "        print(\"************************************************\", file=log_file)\n",
    "\n",
    "        # Calculate and print the sum of changes for each week\n",
    "        for week, col_changes in weekly_counts.items():\n",
    "            weekly_total_changes = sum(col_changes.values())\n",
    "            print(\"\\nTotal Changes for week\", week, \":\", weekly_total_changes, file=log_file)\n",
    "\n",
    "        # Calculate and print the total sum of changes across all weeks\n",
    "        total_changes = {col: 0 for col in cols_to_check_without_date}\n",
    "        for col_changes in weekly_counts.values():\n",
    "            for col, changes in col_changes.items():\n",
    "                if col != '`scrape_date`':\n",
    "                    total_changes[col] += changes\n",
    "        total_sum_changes = sum(total_changes.values())\n",
    "        print(\"\\nTotal Sum of Changes across all weeks:\", total_sum_changes, file=log_file)\n",
    "\n",
    "        total_weeks = len(weekly_counts)\n",
    "        print(\"Total number of weeks:\", total_weeks, file=log_file)\n",
    "\n",
    "        if total_weeks > 0:\n",
    "            print(\"Average changes for all weeks:\", total_sum_changes / total_weeks, file=log_file)\n",
    "        else:\n",
    "            print(\"Average changes for all weeks: 0\", file=log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_report(changed_issue_ids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_report(changed_issue_ids_dict):\n",
    "\n",
    "    monthly_counts = {}  # Dictionary to store monthly counts of changes\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    for index, rows_list in changed_issue_ids_dict.items():\n",
    "\n",
    "        if count % 20000 == 0:\n",
    "            print(\"Iteration reached:\", count)\n",
    "        count = count + 1\n",
    "\n",
    "        returned_specific_issue_data = pd.DataFrame(rows_list, columns=cols_to_check)\n",
    "\n",
    "        # Convert the 'scrape_date' column to datetime format\n",
    "        returned_specific_issue_data['`scrape_date`'] = pd.to_datetime(returned_specific_issue_data['`scrape_date`'])\n",
    "\n",
    "        # Group the data by month\n",
    "        grouped = returned_specific_issue_data.groupby(returned_specific_issue_data['`scrape_date`'].dt.to_period('M'))\n",
    "\n",
    "        # Iterate through each month and its corresponding grouped data\n",
    "        for month, group_data in grouped:\n",
    "            month_str = str(month)\n",
    "            \n",
    "            # Create a dictionary entry for the current month if it doesn't exist\n",
    "            if month_str not in monthly_counts:\n",
    "                monthly_counts[month_str] = {col: 0 for col in cols_to_check_without_date}\n",
    "            \n",
    "            # Calculate the changes in columns for the current month\n",
    "            column_change_sums = {col: 0 for col in cols_to_check_without_date}\n",
    "            prev_row = None\n",
    "            \n",
    "            for idx, row in group_data.iterrows():\n",
    "                if prev_row is not None:\n",
    "                    for col in cols_to_check_without_date:\n",
    "                        if col != '`scrape_date`' and prev_row[col] != row[col]:\n",
    "                            column_change_sums[col] += 1\n",
    "                prev_row = row\n",
    "            \n",
    "            # Update the monthly counts with the changes for this month\n",
    "            for col in cols_to_check_without_date:\n",
    "                monthly_counts[month_str][col] += column_change_sums[col]\n",
    "\n",
    "    with open('real_data_output_GT2_complete.txt', 'a') as log_file:\n",
    "\n",
    "        print(\"************************************************\", file=log_file)\n",
    "\n",
    "        # Calculate and print the sum of changes for each month\n",
    "        for month, col_changes in monthly_counts.items():\n",
    "            monthly_total_changes = sum(col_changes.values())\n",
    "            print(\"\\nTotal Changes for month\", month, \":\", monthly_total_changes, file=log_file)\n",
    "\n",
    "        # Calculate and print the total sum of changes across all months\n",
    "        total_changes = {col: 0 for col in cols_to_check_without_date}\n",
    "        for col_changes in monthly_counts.values():\n",
    "            for col, changes in col_changes.items():\n",
    "                if col != '`scrape_date`':\n",
    "                    total_changes[col] += changes\n",
    "        total_sum_changes = sum(total_changes.values())\n",
    "        print(\"\\nTotal Sum of Changes across all months:\", total_sum_changes, file=log_file)\n",
    "\n",
    "        total_months = len(monthly_counts)\n",
    "        print(\"Total number of months:\", total_months, file=log_file)\n",
    "\n",
    "        if total_months > 0:\n",
    "            print(\"Average changes for all months:\", total_sum_changes / total_months, file=log_file)\n",
    "        else:\n",
    "            print(\"Average changes for all months: 0\", file=log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_report(changed_issue_ids_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
